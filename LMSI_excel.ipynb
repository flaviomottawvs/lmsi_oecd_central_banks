{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753c1b49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804f66c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566db5d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e26ab78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533f8001",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb195c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d912ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57380ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8708787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a625abb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65573721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ae91017",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## VERSAO FINAL , GERA TODOS EM UM MESMO ARQUIVO E JÁ ORDENADOS PELA DATA.\n",
    "## VOCE PODE INTERROMPER O CÓDIGO E EXECUTA-LO DE NOVO QUE ELE CONTINUARÁ O TRABALHO DE ONDE PAROU \n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from openpyxl import Workbook, load_workbook\n",
    "from pdfminer.high_level import extract_text\n",
    "import pyphen\n",
    "from openpyxl.utils import get_column_letter\n",
    "\n",
    "# Inicializando variáveis e downloads necessários\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "url = 'https://drive.google.com/u/0/uc?id=17CmUZM9hGUdGYjCXcjQLyybjTrcjrhik&export=download'\n",
    "loughran_mcdonald_dict = pd.read_csv(url)\n",
    "\n",
    "# Definindo as colunas do dicionário Loughran Mcdonald Sentiment Index\n",
    "word_column = 'Word'\n",
    "category_columns = ['Negative', 'Positive', 'Uncertainty', 'Litigious', 'Constraining', 'Strong_Modal', 'Weak_Modal', 'Syllables']\n",
    "\n",
    "dic = pyphen.Pyphen(lang='en')\n",
    "\n",
    "# Definindo função de análise de sentimento, que usa como parâmetros o texto tokenizado e o dicionário da URL\n",
    "\n",
    "def sentiment_analysis(text, dictionary):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if not word.lower() in stopwords.words('english')]\n",
    "    results = {category: 0 for category in category_columns}\n",
    "    for word in tokens:\n",
    "        if word.upper() in dictionary[word_column].values:\n",
    "            for category in category_columns:\n",
    "                if dictionary.loc[dictionary[word_column] == word.upper(), category].values[0] > 0:\n",
    "                    results[category] += 1\n",
    "    return results\n",
    "\n",
    "# Contagem de sílabas e de palavras \n",
    "\n",
    "def count_syllables(words):\n",
    "    return sum([len(dic.inserted(word).split('-')) for word in words])\n",
    "\n",
    "def count_words_sents_syllables(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    syllables = count_syllables(tokens)\n",
    "    return len(tokens), len(sentences), syllables\n",
    "\n",
    "def calculate_fki(word_count, sent_count, syllable_count):\n",
    "    try:\n",
    "        FKI = 0.39 * (word_count / sent_count) + 11.8 * (syllable_count / word_count) - 15.59\n",
    "        return FKI\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "\n",
    "# Definindo diretórios para salvar o arquivo excel a ser gerado \n",
    "    \n",
    "base_directory = os.getcwd()\n",
    "countries_directory = os.path.join(base_directory, 'PAISES_OCDE')\n",
    "\n",
    "excel_directory = base_directory\n",
    "excel_file = 'LMSI_OCDE_AGRUPADO.xlsx'\n",
    "excel_path = os.path.join(excel_directory, excel_file)\n",
    "\n",
    "# Carregando ou criando a planilha Excel\n",
    "try:\n",
    "    wb = load_workbook(excel_path)\n",
    "    ws = wb.active\n",
    "except FileNotFoundError:\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "    ws.append(['Country', 'Filename'] + category_columns + ['Word Count', 'Sentence Count', 'Syllable Count', 'LMSI', 'FKI'])\n",
    "\n",
    "processed_entries = [(ws['A' + str(row)].value, ws['B' + str(row)].value) for row in range(2, ws.max_row + 1)]\n",
    "\n",
    "\n",
    "\n",
    "for root, dirs, files in os.walk(countries_directory):\n",
    "    country_name = os.path.basename(root)\n",
    "    # Ordenando os arquivos antes do processamento\n",
    "    files = sorted([f for f in files if f.endswith('.pdf') and (country_name, f) not in processed_entries])\n",
    "    for filename in files:\n",
    "        file_path = os.path.join(root, filename)\n",
    "        text = extract_text(file_path)\n",
    "        results = sentiment_analysis(text, loughran_mcdonald_dict)\n",
    "        word_count, sent_count, syllable_count = count_words_sents_syllables(text)\n",
    "        fki_value = calculate_fki(word_count, sent_count, syllable_count)\n",
    "        lmsi = results['Positive'] / (results['Positive'] + results['Negative']) if (results['Positive'] + results['Negative']) > 0 else 0\n",
    "        ws.append([country_name, filename] + [results[category] for category in category_columns] + [word_count, sent_count, syllable_count, lmsi, fki_value])\n",
    "        wb.save(excel_path)  # Salva após cada PDF ser processado\n",
    "        print(f\"Results for {country_name}/{filename} saved to: {excel_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebf63dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
